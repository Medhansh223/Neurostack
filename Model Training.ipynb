{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d558c5-c5c9-439d-8ec6-b9d2a0685408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/medhansh/ikarus/llm_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Text cleaned. Example:\n",
      "['goymfk 1pc free standing shoe rack multi layer metal shoe cap rack 8 double hook living room bathroom hallway goymfk multiple shoe coat hat item easy assemble includes necessary hardware instruction easy assembly versatile perfect use living room bathroom hallway home kitchen storage organization clothing closet storage shoe organizer free standing shoe rack metal white', 'subrtex leather ding room dining chair set 2 black subrtex subrtex dining chair set 2 home kitchen furniture dining room furniture chair sponge black', 'plant repotting mat muyetol waterproof transplanting mat indoor 26 8 x 26 8 portable square foldable easy clean gardening work mat soil changing mat succulent plant transplanting mat garden gift muyetol patio lawn garden outdoor décor doormat polyethylene green']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "atches: 100%|██████████| 10/10 [00:00<00:00, 43.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Generated 312 embeddings of dimension 384\n",
      "\n",
      "Cluster 0:\n",
      "['Kingston Brass BA1752BB Heritage 18-Inch Towel-Bar, Brushed Brass', 'Chief Mfg.Swing-Arm Wall Mount Hardware Mount Black (TS218SU)', 'LASCO 35-5019 Hallmack Style 24-Inch Towel Bar Accessory, All Metal Construction, Chrome Plated Finish']\n",
      "\n",
      "Cluster 1:\n",
      "['Plant Repotting Mat MUYETOL Waterproof Transplanting Mat Indoor 26.8\" x 26.8\" Portable Square Foldable Easy to Clean Gardening Work Mat Soil Changing Mat Succulent Plant Transplanting Mat Garden Gifts', 'Pickleball Doormat, Welcome Doormat Absorbent Non-Slip Floor Mat Bathroom Mat 16x24', 'Plant Repotting Mat MUYETOL Waterproof Transplanting Mat Indoor 26.8\" x 26.8\" Portable Square Foldable Easy to Clean Gardening Work Mat Soil Changing Mat Succulent Plant Transplanting Mat Garden Gifts']\n",
      "\n",
      "Cluster 2:\n",
      "['JOIN IRON Foldable TV Trays for Eating Set of 4 with Stand,Folding TV/Snack Tray Table Set,Folding TV Dinner Tables for Small Space,(Grey)', 'JOIN IRON Foldable TV Trays for Eating Set of 4 with Stand,Folding TV/Snack Tray Table Set,Folding TV Dinner Tables for Small Space,(Grey)', \"Lerliuo Nightstand, Side Table, Industrial Bedside Table with 2 Drawers and Open Shelf, Grey Night Stand, End Table with Steel Frame for Bedroom, Dorm, Gray/Black 23.6''H\"]\n",
      "\n",
      "Cluster 3:\n",
      "['GOYMFK 1pc Free Standing Shoe Rack, Multi-layer Metal Shoe Cap Rack With 8 Double Hooks For Living Room, Bathroom, Hallway', 'Folews Bathroom Organizer Over The Toilet Storage, 4-Tier Bathroom Shelves Over Toilet Shelf Above Toilet Storage Rack Freestanding Bathroom Space Saver Adjustable Shelves and Baskets, Black', 'GOYMFK 1pc Free Standing Shoe Rack, Multi-layer Metal Shoe Cap Rack With 8 Double Hooks For Living Room, Bathroom, Hallway']\n",
      "\n",
      "Cluster 4:\n",
      "['subrtex Leather ding Room, Dining Chairs Set of 2, Black', 'subrtex Leather ding Room, Dining Chairs Set of 2, Black', 'Boss Office Products Any Task Mid-Back Task Chair with Loop Arms in Grey']\n",
      "\n",
      "Cluster 5:\n",
      "['Mellow 2 Inch Ventilated Memory Foam Mattress Topper, Soothing Lavender Infusion, CertiPUR-US Certified, Queen', 'Moroccan Leather Pouf Ottoman for Living Room - Round Leather Ottoman Pouf Cover - Unstuffed Pouf Ottoman Leather Foot Stool - Moroccan Pouf Ottoman - Moroccan Ottoman Pouf Leather (Brown)', '1 Pack Adjustable Height Center Support Leg for Bed Frame,Bed Center Slat Heavy Extra Durable Steel Support Legs | Suitable for Bed Frame,Cabinet and Wooden Furniture']\n",
      "\n",
      "Cluster 6:\n",
      "['LONGWIN Black Hanging Wall Round Mirror Decor Geometric Circle Mirror for Bathroom Bedroom Living Room 10.2inch', 'Plymor Rectangle 5mm Beveled Glass Mirror, 6 inch x 12 inch (Pack of 6)', 'Brightify Black Bathroom Mirrors for Wall, 24 x 36 Inch Rectangle Bathroom Mirrors for Vanity Black Metal Framed Anti-Rust, Matte Black Mirror for Farmhouse Hallway Wall Decor, Horizontal or Vertical']\n",
      "\n",
      "Cluster 7:\n",
      "['Gbuzozie 38L Round Laundry Hamper Cute Mermaid Girl Storage Basket Waterproof Coating Organizer Bin For Nursery Clothes Toys', 'Homebeez 39.1\" Length Bedroom Storage Bench, End Bed Ottoman Bench with Golden Metal Legs, Dressing Chair for Entryway Living Room,Green', 'HomePop Home Decor | K2380-YDQY-2 | Luxury Large Faux Leather Square Storage Ottoman | Ottoman with Storage for Living Room & Bedroom, Dark Brown']\n",
      "\n",
      "Cluster 8:\n",
      "['Flash Furniture Jefferson 2 Pk. Contemporary Brown Vinyl Adjustable Height Barstool with Panel Back and Chrome Base', 'MoNiBloom Adjustable Bar Stools Set of 2, 360° Swivel Counter Height Barstool Kitchen Counter Island Dining Chair, Dark Grey PU Leather Bistro Swivel Stool for Living Room Kitchen, 250 lbs Capacity', 'Homepop Home Decor | Backless Nailhead Trim Counter Height Bar Stools | 24 Inch Bar Stools | Decorative Home Furniture (Blue Tweed)']\n",
      "\n",
      "Cluster 9:\n",
      "[\"LOVMOR 30'' Bathroom Vanity Sink Base Cabine, Storage Cabinet with 3-Drawers on The Left, Suitable for Bathrooms, Kitchens, Laundry Rooms and Other Places.\", \"LOVMOR 30'' Bathroom Vanity Sink Base Cabine, Storage Cabinet with 3-Drawers on The Left, Suitable for Bathrooms, Kitchens, Laundry Rooms and Other Places.\", 'Leick Home 70007-WTGD Mixed Metal and Wood Stepped Tier Bookshelf, White Herringbone/Satin Gold']\n"
     ]
    }
   ],
   "source": [
    "import re, string, nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === Setup NLP tools ===\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    \"\"\"Lowercase, remove punctuation, remove stopwords, lemmatize.\"\"\"\n",
    "    s = str(s).lower()\n",
    "    s = re.sub(f\"[{re.escape(string.punctuation)}]\", \" \", s)\n",
    "    toks = [lemmatizer.lemmatize(w) for w in s.split() if w not in stop_words]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "# === Load Dataset ===\n",
    "df = pd.read_csv('/workspace/medhansh/ikarus/intern_data_ikarus.csv')\n",
    "df.fillna('', inplace=True)\n",
    "\n",
    "# Combine relevant text fields\n",
    "text_fields = ['title', 'brand', 'description', 'categories', 'material', 'color']\n",
    "df['combined_text'] = df[text_fields].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# Clean text using NLP preprocessing\n",
    "df['cleaned_text'] = df['combined_text'].apply(clean_text)\n",
    "\n",
    "print(\"Text cleaned. Example:\")\n",
    "print(df['cleaned_text'].head(3).tolist())\n",
    "\n",
    "# === Create Semantic Text Embeddings ===\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "text_embeddings = model.encode(df['cleaned_text'].tolist(), show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "# Save for backend use\n",
    "np.save('/workspace/medhansh/ikarus/text_embeddings.npy', text_embeddings)\n",
    "df.to_csv('/workspace/medhansh/ikarus/products_text.csv', index=False)\n",
    "\n",
    "print(f\"\\n Generated {text_embeddings.shape[0]} embeddings of dimension {text_embeddings.shape[1]}\")\n",
    "\n",
    "# === NLP Grouping (KMeans Clustering) ===\n",
    "k = 10  # number of semantic groups\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "df['cluster'] = kmeans.fit_predict(text_embeddings)\n",
    "\n",
    "# View top few items from each cluster\n",
    "for i in range(k):\n",
    "    print(f\"\\nCluster {i}:\")\n",
    "    print(df[df['cluster'] == i]['title'].head(3).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "741e0177-d5d0-4659-ab1c-54f90a94c40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Leather At Home, Decorative 13 Inch Rounded Pi...</td>\n",
       "      <td>Leather At Home Store</td>\n",
       "      <td>$26.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Adeco Euro Style Fabric Arm Bench Chair Footst...</td>\n",
       "      <td>Adeco Store</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>PONTMENT Foot Stool Leather Footstool Solid Wo...</td>\n",
       "      <td>PONTMENT</td>\n",
       "      <td>$95.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Black Leather Office Chair Mid Back Leather De...</td>\n",
       "      <td>Arts wish Store</td>\n",
       "      <td>$89.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>AnRui Folding Floor Chair with Adjustable Back...</td>\n",
       "      <td>AnRui Store</td>\n",
       "      <td>$52.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title                  brand  \\\n",
       "158  Leather At Home, Decorative 13 Inch Rounded Pi...  Leather At Home Store   \n",
       "72   Adeco Euro Style Fabric Arm Bench Chair Footst...            Adeco Store   \n",
       "201  PONTMENT Foot Stool Leather Footstool Solid Wo...               PONTMENT   \n",
       "43   Black Leather Office Chair Mid Back Leather De...        Arts wish Store   \n",
       "148  AnRui Folding Floor Chair with Adjustable Back...            AnRui Store   \n",
       "\n",
       "      price  \n",
       "158  $26.49  \n",
       "72           \n",
       "201  $95.99  \n",
       "43   $89.98  \n",
       "148  $52.99  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quick semantic similarity example\n",
    "query = \"modern wooden chair\"\n",
    "q_vec = model.encode([query], normalize_embeddings=True)\n",
    "similarities = cosine_similarity(q_vec, text_embeddings)[0]\n",
    "top_idx = np.argsort(similarities)[::-1][:5]\n",
    "df.iloc[top_idx][['title', 'brand', 'price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba834594-905a-4261-82a4-6ebb93664c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- Clean up any bad hidden unicode characters ---\n",
    "def clean_env(var):\n",
    "    v = os.getenv(var, \"\")\n",
    "    if v:\n",
    "        v = v.encode(\"ascii\", \"ignore\").decode(\"ascii\").strip()\n",
    "        os.environ[var] = v\n",
    "\n",
    "# Reset both Pinecone env vars\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"pcsk_23wW6M_2RHGa3MT5rjdyi2q4oLp3MGYrdF2M5KV3xijynkXFV2DgjYWskCr1AgNw7zaPdk\".strip()\n",
    "os.environ[\"PINECONE_ENV\"] = \"us-east-1\"\n",
    "\n",
    "clean_env(\"PINECONE_API_KEY\")\n",
    "clean_env(\"PINECONE_ENV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff04478-f476-4cc0-a30a-3a6c3f4c5007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Uploaded 100/312 vectors\n",
      "✅ Uploaded 200/312 vectors\n",
      "✅ Uploaded 300/312 vectors\n",
      "✅ Uploaded 312/312 vectors\n",
      "🎯 All embeddings uploaded to Pinecone index: ikarus3d\n"
     ]
    }
   ],
   "source": [
    "# === Store text embeddings in Pinecone ===\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load dataset and embeddings\n",
    "df = pd.read_csv('/workspace/medhansh/ikarus/products_text.csv').fillna('')\n",
    "text_embeddings = np.load('/workspace/medhansh/ikarus/text_embeddings.npy')\n",
    "\n",
    "# Connect to Pinecone\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "# Define index name\n",
    "index_name = \"ikarus3d\"\n",
    "\n",
    "# Create index if not exists\n",
    "if index_name not in [i['name'] for i in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=text_embeddings.shape[1],\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=os.getenv(\"PINECONE_ENV\", \"us-east-1\"))\n",
    "    )\n",
    "\n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "# Prepare and upload data (batch-wise for safety)\n",
    "vectors = []\n",
    "for i, row in df.iterrows():\n",
    "    meta = {\n",
    "        \"uniq_id\": str(row.get(\"uniq_id\", i)),\n",
    "        \"title\": str(row.get(\"title\", \"\")),\n",
    "        \"brand\": str(row.get(\"brand\", \"\")),\n",
    "        \"price\": str(row.get(\"price\", \"\")),\n",
    "        \"categories\": str(row.get(\"categories\", \"\")),\n",
    "        \"material\": str(row.get(\"material\", \"\")),\n",
    "        \"color\": str(row.get(\"color\", \"\")),\n",
    "    }\n",
    "    vectors.append({\n",
    "        \"id\": meta[\"uniq_id\"],\n",
    "        \"values\": text_embeddings[i].tolist(),\n",
    "        \"metadata\": meta\n",
    "    })\n",
    "\n",
    "# Upload in chunks\n",
    "batch_size = 100\n",
    "for i in range(0, len(vectors), batch_size):\n",
    "    batch = vectors[i:i+batch_size]\n",
    "    index.upsert(vectors=batch)\n",
    "    print(f\"Uploaded {i+len(batch)}/{len(vectors)} vectors\")\n",
    "\n",
    "print(\"All embeddings uploaded to Pinecone index:\", index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08fb8732-f76e-49ca-a356-56ac81c8e8f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.540\n",
      "Title: AnRui Folding Floor Chair with Adjustable Back Support, Comfortable, Semi-Foldable, and Versatile, for Meditation, Seminars, Reading, TV Watching or Gaming, Suitable for Home Or Office\n",
      "Brand: AnRui Store\n",
      "Price: $52.99\n",
      "\n",
      "Score: 0.535\n",
      "Title: UTONE Gaming Chair Computer Chair Breathable Fabric Office Chair Cloth with Backrest Desk Chair with Footrest, Lumbar Support Swivel Recliner Task Chair Ergonomic Video Game Chair Height Adjustable\n",
      "Brand: UTONE\n",
      "Price: $199.99\n",
      "\n",
      "Score: 0.524\n",
      "Title: Leather At Home, Decorative 13 Inch Rounded Pillow Handmade from Full Grain Leather - Chair Seat, Confortable Sitting for Round Wooden/Metal Stools - Bourbon Brown\n",
      "Brand: Leather At Home Store\n",
      "Price: $26.49\n",
      "\n",
      "Score: 0.521\n",
      "Title: BOOSDEN Padded Folding Chair 2 Pack, Foldable Chair with Thick Cushion, Heavy Duty Metal Folding Chair for Outdoor & Indoor & Dining & Party, Red\n",
      "Brand: BOOSDEN Store\n",
      "Price: $119.00\n",
      "\n",
      "Score: 0.521\n",
      "Title: MoNiBloom Massage Gaming Recliner Chair with Speakers PU Leather Home Theater Seating Single Bedroom Video Game Sofa Recliners Ergonomic Gaming Couch with Detachable Neck Support and Footrest, Green\n",
      "Brand: MoNiBloom Store\n",
      "Price: \n"
     ]
    }
   ],
   "source": [
    "query = \"chair\"\n",
    "q_vec = model.encode([query], normalize_embeddings=True)[0]\n",
    "\n",
    "# Search top 5\n",
    "result = index.query(vector=q_vec.tolist(), top_k=5, include_metadata=True)\n",
    "\n",
    "for match in result[\"matches\"]:\n",
    "    print(f\"\\nScore: {match['score']:.3f}\")\n",
    "    meta = match[\"metadata\"]\n",
    "    print(f\"Title: {meta['title']}\")\n",
    "    print(f\"Brand: {meta['brand']}\")\n",
    "    print(f\"Price: {meta['price']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cdd7ea-b9e8-40a2-a20a-ea51c3fd4ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPU available: []\n",
      "✅ Using 102 products from top 5 categories:\n",
      "[\"['Patio, Lawn & Garden', 'Outdoor Décor', 'Doormats']\"\n",
      " \"['Home & Kitchen', 'Furniture', 'Living Room Furniture', 'Tables', 'End Tables']\"\n",
      " \"['Home & Kitchen', 'Furniture', 'Living Room Furniture', 'Ottomans']\"\n",
      " \"['Home & Kitchen', 'Home Décor Products', 'Mirrors', 'Wall-Mounted Mirrors']\"\n",
      " \"['Home & Kitchen', 'Furniture', 'Game & Recreation Room Furniture', 'Home Bar Furniture', 'Barstools']\"]\n",
      "\n",
      "📸 Downloading and processing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "00%|██████████| 102/102 [00:06<00:00, 15.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded image array: (102, 224, 224, 3)\n",
      "✅ Classes: [\"['Home & Kitchen', 'Furniture', 'Game & Recreation Room Furniture', 'Home Bar Furniture', 'Barstools']\"\n",
      " \"['Home & Kitchen', 'Furniture', 'Living Room Furniture', 'Ottomans']\"\n",
      " \"['Home & Kitchen', 'Furniture', 'Living Room Furniture', 'Tables', 'End Tables']\"\n",
      " \"['Home & Kitchen', 'Home Décor Products', 'Mirrors', 'Wall-Mounted Mirrors']\"\n",
      " \"['Patio, Lawn & Garden', 'Outdoor Décor', 'Doormats']\"]\n",
      "✅ Train set: (81, 224, 224, 3) Test set: (21, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_4      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_4      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m524,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │         \u001b[38;5;34m1,285\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,113,541</span> (91.99 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,113,541\u001b[0m (91.99 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">525,829</span> (2.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m525,829\u001b[0m (2.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/medhansh/ikarus/llm_env/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from None to 0.66667, saving model to resnet50_best.h5\n",
      "0.3568 - loss: 1.8905"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 720ms/step - accuracy: 0.4444 - loss: 1.6197 - val_accuracy: 0.6667 - val_loss: 0.7976\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.66667 to 0.80952, saving model to resnet50_best.h5\n",
      "850 - loss: 0.4685"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 459ms/step - accuracy: 0.8642 - loss: 0.4353 - val_accuracy: 0.8095 - val_loss: 0.3659\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.80952\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 0.9506 - loss: 0.2464 - val_accuracy: 0.7143 - val_loss: 0.6729\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.80952\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 0.9383 - loss: 0.2301 - val_accuracy: 0.7619 - val_loss: 0.6399\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_accuracy improved from 0.80952 to 0.90476, saving model to resnet50_best.h5\n",
      "870 - loss: 0.0594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 461ms/step - accuracy: 0.9506 - loss: 0.1357 - val_accuracy: 0.9048 - val_loss: 0.3936\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 445ms/step - accuracy: 1.0000 - loss: 0.0791 - val_accuracy: 0.7619 - val_loss: 0.4961\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 0.9753 - loss: 0.1005 - val_accuracy: 0.8095 - val_loss: 0.4679\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 0.9753 - loss: 0.0622 - val_accuracy: 0.8095 - val_loss: 0.5232\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 465ms/step - accuracy: 0.9877 - loss: 0.0434 - val_accuracy: 0.7619 - val_loss: 0.5318\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0153 - val_accuracy: 0.8095 - val_loss: 0.4970\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0220 - val_accuracy: 0.8095 - val_loss: 0.4621\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 435ms/step - accuracy: 1.0000 - loss: 0.0187 - val_accuracy: 0.7619 - val_loss: 0.5749\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 434ms/step - accuracy: 1.0000 - loss: 0.0306 - val_accuracy: 0.7143 - val_loss: 0.6107\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 450ms/step - accuracy: 1.0000 - loss: 0.0200 - val_accuracy: 0.7619 - val_loss: 0.6994\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.7143 - val_loss: 0.7014\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0157 - val_accuracy: 0.7143 - val_loss: 0.5923\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 434ms/step - accuracy: 1.0000 - loss: 0.0077 - val_accuracy: 0.7143 - val_loss: 0.6338\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0109 - val_accuracy: 0.7619 - val_loss: 0.5962\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 433ms/step - accuracy: 1.0000 - loss: 0.0086 - val_accuracy: 0.7619 - val_loss: 0.5298\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0066 - val_accuracy: 0.8095 - val_loss: 0.4685\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0056 - val_accuracy: 0.8095 - val_loss: 0.4101\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 0.8095 - val_loss: 0.4196\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0030 - val_accuracy: 0.8095 - val_loss: 0.4504\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.8095 - val_loss: 0.4336\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 430ms/step - accuracy: 1.0000 - loss: 0.0066 - val_accuracy: 0.8095 - val_loss: 0.4527\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 0.9877 - loss: 0.0202 - val_accuracy: 0.7619 - val_loss: 0.6175\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 0.7619 - val_loss: 0.7213\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0058 - val_accuracy: 0.7619 - val_loss: 0.5792\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 454ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.7619 - val_loss: 0.4474\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0079 - val_accuracy: 0.7619 - val_loss: 0.4625\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 430ms/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.8095 - val_loss: 0.3814\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 444ms/step - accuracy: 1.0000 - loss: 0.0028 - val_accuracy: 0.8095 - val_loss: 0.3440\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 433ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.7619 - val_loss: 0.5105\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 34: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0169 - val_accuracy: 0.7619 - val_loss: 0.5314\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 35: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 415ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.8095 - val_loss: 0.3535\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 36: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.7619 - val_loss: 0.4995\n",
      "Epoch 37/100\n",
      "\n",
      "Epoch 37: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 433ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.6667 - val_loss: 0.8810\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 38: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 429ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 0.6667 - val_loss: 0.8654\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 39: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.7619 - val_loss: 0.5165\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 40: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.8095 - val_loss: 0.3450\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 41: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0067 - val_accuracy: 0.8095 - val_loss: 0.3897\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 42: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0033 - val_accuracy: 0.8095 - val_loss: 0.5524\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 43: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.7619 - val_loss: 0.5817\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 44: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 415ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.7619 - val_loss: 0.5175\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 45: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.7619 - val_loss: 0.4966\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 46: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 433ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.7143 - val_loss: 0.5217\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 47: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 8.8913e-04 - val_accuracy: 0.7143 - val_loss: 0.5573\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 48: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 7.6654e-04 - val_accuracy: 0.7143 - val_loss: 0.5599\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 49: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 9.7224e-04 - val_accuracy: 0.7143 - val_loss: 0.5388\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 50: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 429ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.7619 - val_loss: 0.5198\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 51: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.8095 - val_loss: 0.4939\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 52: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 9.6214e-04 - val_accuracy: 0.8095 - val_loss: 0.5097\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 53: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 420ms/step - accuracy: 1.0000 - loss: 9.0596e-04 - val_accuracy: 0.7143 - val_loss: 0.5285\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 54: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 8.0071e-04 - val_accuracy: 0.7143 - val_loss: 0.5096\n",
      "Epoch 55/100\n",
      "\n",
      "Epoch 55: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 415ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.8095 - val_loss: 0.5041\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 56: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.7619 - val_loss: 0.4930\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 57: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 0.9877 - loss: 0.0264 - val_accuracy: 0.7619 - val_loss: 0.7165\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 58: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0095 - val_accuracy: 0.7619 - val_loss: 0.9628\n",
      "Epoch 59/100\n",
      "\n",
      "Epoch 59: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0037 - val_accuracy: 0.7143 - val_loss: 0.5807\n",
      "Epoch 60/100\n",
      "\n",
      "Epoch 60: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 454ms/step - accuracy: 1.0000 - loss: 0.0057 - val_accuracy: 0.7143 - val_loss: 0.4898\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 61: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0089 - val_accuracy: 0.7143 - val_loss: 0.8585\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 62: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0071 - val_accuracy: 0.7143 - val_loss: 0.9982\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 63: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 445ms/step - accuracy: 1.0000 - loss: 0.0039 - val_accuracy: 0.7143 - val_loss: 0.7851\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 64: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 416ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7143 - val_loss: 0.6388\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 65: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 7.7840e-04 - val_accuracy: 0.7143 - val_loss: 0.6168\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 66: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.7143 - val_loss: 0.6543\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 67: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 444ms/step - accuracy: 1.0000 - loss: 0.0021 - val_accuracy: 0.7143 - val_loss: 0.6647\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 68: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 426ms/step - accuracy: 1.0000 - loss: 5.0841e-04 - val_accuracy: 0.7619 - val_loss: 0.6374\n",
      "Epoch 69/100\n",
      "\n",
      "Epoch 69: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 9.8051e-04 - val_accuracy: 0.8095 - val_loss: 0.5317\n",
      "Epoch 70/100\n",
      "\n",
      "Epoch 70: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 426ms/step - accuracy: 1.0000 - loss: 0.0027 - val_accuracy: 0.8095 - val_loss: 0.4328\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 71: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.8095 - val_loss: 0.4654\n",
      "Epoch 72/100\n",
      "\n",
      "Epoch 72: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 434ms/step - accuracy: 1.0000 - loss: 8.8800e-04 - val_accuracy: 0.8095 - val_loss: 0.5154\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 73: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 445ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.7619 - val_loss: 0.5457\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 74: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 5.6528e-04 - val_accuracy: 0.7619 - val_loss: 0.5221\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 75: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 9.8173e-04 - val_accuracy: 0.7619 - val_loss: 0.4570\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 76: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 420ms/step - accuracy: 1.0000 - loss: 8.3694e-04 - val_accuracy: 0.8095 - val_loss: 0.3866\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 77: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.8095 - val_loss: 0.4070\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 78: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 420ms/step - accuracy: 1.0000 - loss: 5.1842e-04 - val_accuracy: 0.8095 - val_loss: 0.4518\n",
      "Epoch 79/100\n",
      "\n",
      "Epoch 79: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 4.4411e-04 - val_accuracy: 0.8095 - val_loss: 0.5123\n",
      "Epoch 80/100\n",
      "\n",
      "Epoch 80: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.8095 - val_loss: 0.6883\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 81: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 3.5775e-04 - val_accuracy: 0.8095 - val_loss: 0.7189\n",
      "Epoch 82/100\n",
      "\n",
      "Epoch 82: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.8095 - val_loss: 0.5667\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 83: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 3.5359e-04 - val_accuracy: 0.8095 - val_loss: 0.4531\n",
      "Epoch 84/100\n",
      "\n",
      "Epoch 84: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 2.4177e-04 - val_accuracy: 0.8095 - val_loss: 0.4291\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 85: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 6.0507e-04 - val_accuracy: 0.8095 - val_loss: 0.4499\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 86: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 0.7619 - val_loss: 0.6252\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 87: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 2.9652e-04 - val_accuracy: 0.7143 - val_loss: 0.7136\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 88: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 8.0411e-04 - val_accuracy: 0.7143 - val_loss: 0.7082\n",
      "Epoch 89/100\n",
      "\n",
      "Epoch 89: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 415ms/step - accuracy: 1.0000 - loss: 5.4201e-04 - val_accuracy: 0.8095 - val_loss: 0.6509\n",
      "Epoch 90/100\n",
      "\n",
      "Epoch 90: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 7.6046e-04 - val_accuracy: 0.8095 - val_loss: 0.6036\n",
      "Epoch 91/100\n",
      "\n",
      "Epoch 91: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 3.6649e-04 - val_accuracy: 0.8095 - val_loss: 0.5519\n",
      "Epoch 92/100\n",
      "\n",
      "Epoch 92: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 415ms/step - accuracy: 1.0000 - loss: 2.0337e-04 - val_accuracy: 0.8095 - val_loss: 0.4944\n",
      "Epoch 93/100\n",
      "\n",
      "Epoch 93: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 429ms/step - accuracy: 1.0000 - loss: 5.3933e-04 - val_accuracy: 0.8095 - val_loss: 0.4467\n",
      "Epoch 94/100\n",
      "\n",
      "Epoch 94: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 3.0943e-04 - val_accuracy: 0.8095 - val_loss: 0.4149\n",
      "Epoch 95/100\n",
      "\n",
      "Epoch 95: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 3.2371e-04 - val_accuracy: 0.8095 - val_loss: 0.4196\n",
      "Epoch 96/100\n",
      "\n",
      "Epoch 96: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 1.3776e-04 - val_accuracy: 0.8095 - val_loss: 0.4244\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 97: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 424ms/step - accuracy: 1.0000 - loss: 4.7466e-04 - val_accuracy: 0.8095 - val_loss: 0.4839\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 98: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 434ms/step - accuracy: 1.0000 - loss: 1.9873e-04 - val_accuracy: 0.8095 - val_loss: 0.4985\n",
      "Epoch 99/100\n",
      "\n",
      "Epoch 99: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 434ms/step - accuracy: 1.0000 - loss: 2.7274e-04 - val_accuracy: 0.8095 - val_loss: 0.4910\n",
      "Epoch 100/100\n",
      "\n",
      "Epoch 100: val_accuracy did not improve from 0.90476\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 425ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.8095 - val_loss: 0.4572\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 746ms/step - accuracy: 0.8095 - loss: 0.4572\n",
      "✅ Test Accuracy: 80.95%\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ff75433a0c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ff75433a0c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Classification Report:\n",
      "                                                                                                        precision    recall  f1-score   support\n",
      "\n",
      "['Home & Kitchen', 'Furniture', 'Game & Recreation Room Furniture', 'Home Bar Furniture', 'Barstools']       0.80      1.00      0.89         4\n",
      "                                  ['Home & Kitchen', 'Furniture', 'Living Room Furniture', 'Ottomans']       0.57      1.00      0.73         4\n",
      "                      ['Home & Kitchen', 'Furniture', 'Living Room Furniture', 'Tables', 'End Tables']       1.00      0.25      0.40         4\n",
      "                          ['Home & Kitchen', 'Home Décor Products', 'Mirrors', 'Wall-Mounted Mirrors']       1.00      1.00      1.00         4\n",
      "                                                 ['Patio, Lawn & Garden', 'Outdoor Décor', 'Doormats']       1.00      0.80      0.89         5\n",
      "\n",
      "                                                                                              accuracy                           0.81        21\n",
      "                                                                                             macro avg       0.87      0.81      0.78        21\n",
      "                                                                                          weighted avg       0.88      0.81      0.79        21\n",
      "\n",
      "✅ Final model saved as resnet50_product_classifier_final.h5\n",
      "✅ Best model saved as resnet50_best.h5\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. Imports\n",
    "# ============================================\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# ============================================\n",
    "# 2. Load Dataset\n",
    "# ============================================\n",
    "df = pd.read_csv(\"/workspace/medhansh/ikarus/products_text.csv\")\n",
    "\n",
    "# Filter top 5 categories\n",
    "top5 = df['categories'].value_counts().nlargest(5).index\n",
    "df = df[df['categories'].isin(top5)].reset_index(drop=True)\n",
    "print(\"Using\", len(df), \"products from top 5 categories:\")\n",
    "print(df['categories'].unique())\n",
    "\n",
    "# ============================================\n",
    "# 3. Download & preprocess images\n",
    "# ============================================\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "print(\"\\n📸 Downloading and processing images...\")\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    raw_urls = row['images']\n",
    "    label = row['categories']\n",
    "\n",
    "    # Parse list of URLs if needed\n",
    "    try:\n",
    "        url_list = ast.literal_eval(raw_urls) if isinstance(raw_urls, str) else raw_urls\n",
    "        if isinstance(url_list, list) and len(url_list) > 0:\n",
    "            url = url_list[0].strip()  # pick first URL\n",
    "        else:\n",
    "            continue\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    # Download image\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        img = Image.open(BytesIO(r.content)).convert(\"RGB\")\n",
    "        img = img.resize((224, 224))  # ResNet50 input size\n",
    "        img_arr = np.array(img)\n",
    "        images.append(img_arr)\n",
    "        labels.append(label)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {url}: {e}\")\n",
    "\n",
    "X = np.array(images)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(\"Loaded image array:\", X.shape)\n",
    "\n",
    "# ============================================\n",
    "# 4. Encode labels\n",
    "# ============================================\n",
    "if len(y) == 0:\n",
    "    raise ValueError(\"No images were loaded. Check URLs or dataset format.\")\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "print(\"Classes:\", le.classes_)\n",
    "\n",
    "# ============================================\n",
    "# 5. Train/Test Split\n",
    "# ============================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.2, random_state=42, stratify=y_enc\n",
    ")\n",
    "\n",
    "# Preprocess for ResNet50\n",
    "X_train = preprocess_input(X_train)\n",
    "X_test = preprocess_input(X_test)\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"Train set:\", X_train.shape, \"Test set:\", X_test.shape)\n",
    "\n",
    "# ============================================\n",
    "# 6. Build Transfer Learning Model (ResNet50)\n",
    "# ============================================\n",
    "base_model = ResNet50(\n",
    "    weights='imagenet', include_top=False, input_shape=(224, 224, 3)\n",
    ")\n",
    "base_model.trainable = False  # Freeze base layers\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# ============================================\n",
    "# 7. Save Best Model Callback\n",
    "# ============================================\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    filepath=\"resnet50_best.h5\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    save_best_only=True,\n",
    "    mode=\"max\",\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 8. Data Augmentation\n",
    "# ============================================\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# ============================================\n",
    "# 9. Train the Model\n",
    "# ============================================\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=8),\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    epochs=100,\n",
    "    callbacks=[checkpoint_cb]\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# 10. Evaluate & Classification Report\n",
    "# ============================================\n",
    "loss, acc = model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# Predict classes\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# ============================================\n",
    "# 11. Save Final Model\n",
    "# ============================================\n",
    "model.save(\"resnet50_product_classifier_final.h5\")\n",
    "print(\"Final model saved as resnet50_product_classifier_final.h5\")\n",
    "print(\"Best model saved as resnet50_best.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574000d-5a59-4b26-8634-1f21ff98e45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Best checkpoint — Test accuracy: 85.71%\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ff71436af20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7ff71436af20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step   \n",
      "\n",
      "📊 Classification Report:\n",
      "                                                                                                        precision    recall  f1-score   support\n",
      "\n",
      "['Home & Kitchen', 'Furniture', 'Game & Recreation Room Furniture', 'Home Bar Furniture', 'Barstools']     1.0000    1.0000    1.0000         4\n",
      "                                  ['Home & Kitchen', 'Furniture', 'Living Room Furniture', 'Ottomans']     0.6667    0.5000    0.5714         4\n",
      "                      ['Home & Kitchen', 'Furniture', 'Living Room Furniture', 'Tables', 'End Tables']     1.0000    1.0000    1.0000         4\n",
      "                          ['Home & Kitchen', 'Home Décor Products', 'Mirrors', 'Wall-Mounted Mirrors']     0.6667    1.0000    0.8000         4\n",
      "                                                 ['Patio, Lawn & Garden', 'Outdoor Décor', 'Doormats']     1.0000    0.8000    0.8889         5\n",
      "\n",
      "                                                                                              accuracy                         0.8571        21\n",
      "                                                                                             macro avg     0.8667    0.8600    0.8521        21\n",
      "                                                                                          weighted avg     0.8730    0.8571    0.8538        21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Load best checkpoint\n",
    "best = load_model(\"/workspace/medhansh/ikarus/resnet50_best.h5\")\n",
    "\n",
    "# Preprocess test set (ResNet50)\n",
    "X_test_pp = preprocess_input(X_test.astype(np.float32))\n",
    "\n",
    "# Evaluate accuracy\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes=len(le.classes_))\n",
    "loss, acc = best.evaluate(X_test_pp, y_test_cat, verbose=0)\n",
    "print(f\"Best checkpoint — Test accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "y_pred = best.predict(X_test_pp, batch_size=8)\n",
    "y_pred_labels = y_pred.argmax(axis=1)\n",
    "print(\"\\n📊 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_labels, target_names=list(le.classes_), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0809fb-af81-4910-b59d-7dae58c20b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded model and classes: [\"['Home & Kitchen', 'Furniture', 'Game & Recreation Room Furniture', 'Home Bar Furniture', 'Barstools']\"\n",
      " \"['Home & Kitchen', 'Furniture', 'Living Room Furniture', 'Ottomans']\"\n",
      " \"['Home & Kitchen', 'Furniture', 'Living Room Furniture', 'Tables', 'End Tables']\"\n",
      " \"['Home & Kitchen', 'Home Décor Products', 'Mirrors', 'Wall-Mounted Mirrors']\"\n",
      " \"['Patio, Lawn & Garden', 'Outdoor Décor', 'Doormats']\"]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model(\"resnet50_best.h5\")\n",
    "classes = np.load(\"label_classes.npy\", allow_pickle=True)\n",
    "print(\"Loaded model and classes:\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d3afb-35af-4429-bfde-c1caac66c92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "✅ Predicted class: ['Home & Kitchen', 'Furniture', 'Living Room Furniture', 'Ottomans']\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Path to new image\n",
    "img_path = \"/workspace/medhansh/ikarus/chair.jpeg\"\n",
    "\n",
    "# Load & preprocess the image\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img = img.resize((224, 224))\n",
    "img_array = np.array(img)\n",
    "\n",
    "# Add batch dimension and preprocess\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array = preprocess_input(img_array)\n",
    "\n",
    "# Predict\n",
    "pred = model.predict(img_array)\n",
    "pred_class_index = np.argmax(pred, axis=1)[0]\n",
    "pred_class_name = classes[pred_class_index]\n",
    "\n",
    "print(\"Predicted class:\", pred_class_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLM Env)",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
